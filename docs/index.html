<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Simulating Datacenter Fabrics</title>
        <link rel="icon" type="image/x-icon" href="assets/img/datacenter.png" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.12.1/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="#page-top"><h1>Simulating Datacenter<br>Frameworks</h1></a><button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">Menu<i class="fas fa-bars ml-1"></i></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ml-auto">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="file:html/index.html">Documentation</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#overview">Overview</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#design">Design</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#results">Results</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#conclusion">Conclusion</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <header class="masthead">
            <div class="container">
                <div class="masthead-heading text-uppercase">Simulating Datacenter Fabrics</div>
                <div class="masthead-subheading">
                    Harvard CS205 Final Project - Spring 2020 <br>
                    Group 6: Erik Johnsson, Muhammad Tirmazi, Jessica Wijaya, and Ivan Zhang <br>
                    <a href="https://github.com/ivzhang2/simulating-datacenter-fabrics">GitHub Repository<br><img src="assets/img/github.png"></a>

                </div>

            </div>
        </header>
        <!-- Services-->
        <section class="page-section" id="overview">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Overview</h2>
                    <h3 class="section-subheading">Background</h3>
                </div>
                    <p>
                        Cloud vendors, including Amazon AWS, Microsoft Azure and Google Cloud, 
                        make their datacenter network accessible for clients by abstracting out 
                        the complexities of the underlying interconnect. For the client, the 
                        datacenter network is one large fabric that magically connects all of their 
                        rented instances as in <b>Figure 1</b>. <br>
                        <div class="text-center">
                            <img src="assets/img/figure1.png">
                        </div>
                        An actual data center fabric, however, looks much more like <b>Figure 2</b> below. 
                        The process of designing and deploying a data center fabric, and its accompanying 
                        network stack, costs upwards of a billion dollars. Hence, computer scientists 
                        evaluate a potential designâ€™s performance using a network simulator prior to 
                        deploying it in an actual datacenter. 
                        <div class="text-center">
                            <img src="assets/img/figure2.png">
                        </div>
                    </p>
                <div class="text-center">
                    <h3 class="section-subheading">Problem</h3>
                </div>
                    <p>
                        Network simulators are usually serial. This proves to be a major issue for data center
                        frameworks which tend to be massive in scale with multiple highly interconnected
                        layers. Furthermore for large data centers, which are extremely expensive, network
                        issues like network congestion can result in additional high costs and service issues
                        if unaccounted for. Therefore these simulations are important as they help highlight issues
                        such as network bottlenecks in different network topologies. These simulations also help to
                        optimize network topologies and better balance traffic. We will use parallelization to deal
                        with the <b>compute-intensive</b> aspect of traversing network nodes (i.e. servers, switches, and links)
                        that comprise these simulations.
                        <br><br>
                        To get an intuition for why parallelizing a 
                        network is non-trivial, consider the following naive way of dividing the problem.
                        <br><br>
                        Approach 1: Simulate network devices in parallel. This involves writing a simulator for 
                        a network switch and running parallel simulations for all the switches in figure 2 on 
                        separate threads. The problem with this approach is that there is a dependence between 
                        a switch and every other switch in the network. As an example, you cannot simulate the 
                        load on any of the switches at the top of figure 2 (core) without knowing how much data 
                        they are being sent by the switches underneath them (aggregation). The same goes for 
                        any other set of switches one can think of.
                        <br><br>
                        Other approaches, such as trying to simulate network traffic in parallel, have similar pitfalls.
                    </p>
                <div class="text-center">
                    <h3 class="section-subheading">Solution</h3>
                </div>
                <p>
                    For our project, we used multi-processing for the purposes of traffic generation and for reading traces,
                    traffic matrices, and network topologies. Furthermore, we used multi-threading for the execution of 
                    pruned/isolated network clusters on separate nodes. Finally shared memory is used to aggregate the results
                    at the end as well as to share traffic information between nodes.
                </p>
            </div>
        </section>
        <!-- Portfolio Grid-->
        <section class="page-section bg-light" id="design">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Design</h2>
                    <div class="text-center">
                        <h3 class="section-subheading">Overview of Phases</h3>
                    </div>
                </div>
                <p>
                    The design of our project consisted of 3 phases.
                    <br><br>
                    <b>Phase 1</b>: Fake traffic generation for network flow. <br>
                    In order to generate fake traffic, we require user input in the form of the CDF of the traffic
                    as well as the interarrival times. With the CDF, we use random sampling on the inverse CDF in order
                    to simulate the load of the time. Finally, we use the Poisson Process in order to get the time of the
                    next flow.
                    <br><br>
                    <b>Phase 2</b>: Fake network creation with switches and service/time delay for simulation. <br>
                    For this phase, we implemented a M/M/1 Queue that simulated the service time of the switches. This
                    queue is then processed in a FIFO manner. While this queue is processed, we add the link delay between
                    switches as the network simulation runs. This queue will run through the network topology created in 
                    the next phase.
                    <br><br>
                    <b>Phase 3</b>: Evaluation of network topologies using simulation. <br>
                    We realize that network packets don't always interact with every single part of the network. Thus we can
                    parallelize the network through the creation of isolated network clusters. Thus in this phase, we 
                    prune the entire network into network clusters that are isolated from one another 
                    (i.e. packets don't move from one network cluster to another). In this way, we can parallelize flows
                    through each of these network clusters and speedup the performance of the simulation.
                    <br><br>
                    Phase 1 and Phase 2 are combined into the "Traffic Generation" component of our project and Phase 3 
                    is the "Network Simulation" component of our project.
                    <br>
                    At the end of our project, we produce a flow completion time CDF like the one shown below (which was 
                    generated from a "fork" topology of one sender and two receivers). 
                    <div class="text-center">
                        <img src="../results/fct_cdf.png" style="width:50%">
                    </div>
                </p>
                <div class="text-center">
                    <h3 class="section-subheading">Network Topology</h3>
                </div>
                <p>
                    For our network simulation, we chose to create a fat tree topology. A graphical representation can be found
                    below.
                    <div class="text-center">
                        <img src="../images/fattree.png" style="width:50%">
                    </div>
                    <br>
                    Using our test.topo.dot file (fat_tree.topo found on the GitHub repository under tests/), we generated our
                    own fat tree network topology shown below.
                    <br>
                    <div class="text-center">
                        <img src="assets/img/fatree.png" style="width:100%">
                    </div>
                    <br>
                    In the diagram, the network instances are labeled as "h#" for hosts/servers and "s#" for switches. The 
                    lines connecting the network instances are the links in the network.
                </p>
                <div class="text-center">
                    <h3 class="section-subheading">Data Used</h3>
                </div>
                <p>
                    Examples of network flow size traces can be found publicly from 
                    <a href="https://www.caida.org/data/passive/passive_dataset_download.xml" style="color:#17a2b8">CAIDA (Center for Applied Internet Data Analysis)</a>,
                    <a href="https://github.com/datacenter/empirical-traffic-gen" style="color:#17a2b8">GitHub</a>, and
                    <a href="https://github.com/google/cluster-data/blob/master/ClusterData2019.md" style="color:#17a2b8">Google</a>.
                    <br> 
                    More information on the use of the Poisson Process for network load generation can be found on  
                    <a href="https://web.stanford.edu/~skatti/pubs/sigcomm13-pfabric.pdf" style="color:#17a2b8">Alizadeh et. al (MIT)</a>, and
                    <a href="https://www.cs.cmu.edu/~harchol/PerformanceModeling/book.html" style="color:#17a2b8">Harchol-Balter et. al (Carnegie Mellon)
                    </a>.
                </p>
                <div class="text-center">
                    <h3 class="section-subheading">Infrastructure</h3>
                </div>
                <p>
                    <div class="text-center">
                        <img class="side-image" src="assets/img/MPI.png"><img class="side-image" src="assets/img/openMP.png">
                    </div>
                    The main 2 parallel infrastructures used are MPI and OpenMP. Specifically OpenMP was used for the Traffic
                    Generation (Phase 1 + 2) in order to speed up the process of network creation, previously a serial task.
                    MPI was used for Network Simulation (Phase 3) as a way for the pruned network clusters on separate nodes
                    to communicate with one another.
                </p>
                <div class="text-center">
                    <h3 class="section-subheading">Specification</h3>
                </div>
                <p>
                    <b>Machine Specifications</b>
                    <ul>
                        <li>SPECIFICATIONS</li>
                        <li>SPECIFICATIONS</li>
                        <li>SPECIFICATIONS</li>
                        <li>SPECIFICATIONS</li>
                    </ul>
                    <b>Software Specifications</b>
                    <ul>
                        <li>SPECIFICATIONS</li>
                        <li>SPECIFICATIONS</li>
                        <li>SPECIFICATIONS</li>
                        <li>SPECIFICATIONS</li>
                    </ul>
                </p>
            </div>
        </section>
        <!-- About-->
        <section class="page-section" id="results">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Results</h2>
                    <h3 class="section-subheading">Traffic Generation</h3>
                </div>
                <p>
                    We used OpenMP to parallelize the traffic generation code on an increasing amount of threads
                    alongside an increasing amount of flow sizes to be generated. These flow sizes increased in a
                    exponential manner from 0.1 to 1 and eventually to 10000.
                    <br><br> 
                    Bar plots for the traffic generation phases can be found below (left is on a linear scale 
                    and right is on a logarithmic scale). Furthermore, these bar plots display the error margins.
                    <div class="text-center">
                        <img class="side-image" src="../images/traffic_generation_barplot.png" style="width:50%"><img class="side-image" src="../images/traffic_generation_barplot_log.png" style="width:50%">
                    </div>
                    From the data, it is evident that paralleization of the traffic generation provided little to no improvement
                    when the flow size to be generated is 0.1. Furthermore, the improvements that results from the parallelization
                    of flow size generation of sizes 1, 10, and 100 are minor and are only visible on a logarithmic scale. Greater 
                    improvements from paralleization result from flow size generation greater that 100 (i.e. 1000 and 10,000). 
                    <br><br>
                    Furthermore, it can be seen that in most cases more threads improves completion time. The exceptions to this
                    are when flow sizes are 0.1, 1, and even 10 (when increasing from 4 to 5 threads), although the deviations are
                    minor in scale.
                    <br><br>
                    To better visualize the speedup with an increasing number of threads, we plotted the speedup from an increasing
                    number of threads alongside an increasing flow size.
                    <div class="text-center">
                        <img src="../images/traffic_generation_lineplot.png" style="width:50%">
                    </div>
                    From the line plot, speedup with 1 thread is constant, as it should be. Furthermore, speedup increases alongside
                    an increasing number of threads (although it should be noted that speedup is relatively the same for any number of
                    threads when flow size is less than or equal to 10). The rate of speedup growth is higher for runs with more threads. Finally, 
                    the rate of speedup growth seems to approach zero for flow sizes above 1000.
                </p>
                <div class="text-center">
                    <h3 class="section-subheading">Network Simulation</h3>
                </div>
                <p>
                    We used MPI to parallelize the network simulation code on 1 MPI node and 4 MPI nodes for increasing sizes of flows that were
                    generated in the Traffic Generation aspect of our code.
                    <br><br> 
                    Bar plots for the network simulation phase can be found below.
                    <div class="text-center">
                        <img src="../images/simulation_completion_time.png" style="width:50%">
                    </div>
                    From the bar plot, it is evident that, in most cases, parallelization with MPI significantly improves the completion time of the 
                    simulation. The exceptions for this are when the flow size is 1, where completion time of the parallelized run is slower, and
                    when the flow size is 5, where the completion time of the parallelized run is only slightly faster.
                    <br><br>
                    To better visualize the speedup with more MPI nodes, we plotted the speedup from using 4 MPI nodes
                    alongside increasing flow size.
                    <div class="text-center">
                        <img src="../images/simulation_speedup.png" style="width:50%">
                    </div>
                    From the line plot above, it can be seen that speedup grows linearly with problem size but declines slightly
                    with flow sizes greater than 25. It should also be noted that for flow sizes between 1 and 5, speedup is less
                    than one, meaning that the parallel version is slower than the serial version.
                </p>
            </div>
        </section>
        <!-- Team-->
        <section class="page-section bg-light" id="conclusion">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Conclusion</h2>
                    <h3 class="section-subheading">Challenges</h3>
                </div>
                <p>
                    The main challenges we faced were:
                    <ul>
                        <li>Ensuring that network clusters are correctly pruned and isolated.</li>
                        <li>Creating small yet comprehensive data structures to hold what is needed for the simulation</li>
                        <li>Combining every single phase (Traffic Generation + Network Simulation) to create a seamless application</li>
                        <li>Optimizing the parallelized performance of the nodes for the network simulation.</li>
                    </ul>
                </p>
                <div class="text-center">
                    <h3 class="section-subheading">Overheads</h3>
                </div>
                <p>
                    The main overheads of our project were:
                    <ul>
                        <li>
                            Communication: Multiple objects in our code (i.e. switches, links, and queues) needed
                            to communicate with one another to relay information and pass the network packet.
                            Furthermore, communication was needed between the MPI nodes in order to communicate
                            results and times.
                        </li>
                        <li>
                            I/O: The need to read and write large files for the random sampling of the CDF,
                            the generation of the fat tree network topology, and reading the flow sizes for the simulation.
                        </li>
                        <li>
                            Sequential Operations: The queue used by the server is serial by nature and cannot be parallelized.
                        </li>
                        <li>
                            Load Balancing: Time is needed in order to prune and prepare the isolated network clusters that can 
                            then be used for parallelization.
                        </li>
                    </ul>
                </p>
                <div class="text-center">
                    <h3 class="section-subheading">Conclusions</h3>
                </div>
                <p>
                    Parallelization of our Traffic Generation code significantly benefitted it performance although the rate
                    of its speedup increases started to decline after flow sizes over 1000. It should be noted, however, that
                    parallelization only helped Traffic Generations of flow sizes over 100 due to the added overheads that
                    comes with parallelization. Thus, we suggest that parallelization by increasing threads with OpenMP 
                    is a viable way of generating the large traffic flows needed for network simulations. One caveat, however,
                    is that speedup starts to remain constant after flow sizes of 1000 which could be addressed with more threads
                    or other improvements in the code (see Future Work below).
                    <br><br>
                    Parallelization of our Network Simulation with just 4 MPI nodes resulted in massive improvements in performance,
                    although speedup started to decline after flow sizes of 30. It should also be noted that (like the parallelization
                    of the traffic generation), parallelization provided little benefits and was, at times, detrimental to completion
                    time for flow sizes at or below 5. Thus, we suggest that parallelization through the use of MPI nodes is viable
                    way to reduce the completion time of large network simulations. The benefits/speedups of this parallelization 
                    could be potentially greater with the use of more MPI nodes (see Future Work below).
                </p>
                <div class="text-center">
                    <h3 class="section-subheading">Future Work</h3>
                </div>
                <p>
                    In the future, we would like to investigate whether an increasing number of MPI nodes will further speedup
                    the simulation. Furthermore, we would like to look into the posibility of GPU computing as a way of improving
                    our traffic generation as well as network simulation code. Additionally, we can find methods to deal with overheads
                    that hinder the speed of some aspects of our project.
                </p>
            </div>
        </section>

        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Contact form JS-->
        <script src="assets/mail/jqBootstrapValidation.js"></script>
        <script src="assets/mail/contact_me.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
